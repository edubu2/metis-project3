{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "instrumental-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from splits import split_users # contains split_users func\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 101)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expressed-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in pickled DFs generated by query_dfs.py\n",
    "\n",
    "# df_orders = pd.read_pickle(\"./pickle/df_orders.pickle\")\n",
    "# df_train = pd.read_pickle(\"./pickle/df_train.pickle\")\n",
    "# df_prior = pd.read_pickle(\"./pickle/df_prior.pickle\")\n",
    "# df_prod_detail = pd.read_pickle(\"./pickle/df_prod_detail.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equipped-control",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['product_id', 'user_id', 'user_total_prod_orders', 'cart', 'in_cart',\n",
       "       'last_cart', 'in_last_cart', 'qty_reordered', 'qty_sold',\n",
       "       'prod_reorder_pct', 'prod_prior_sales', 'prod_pct_reorders',\n",
       "       'prod_avg_atco', 'user_avg_cart_size', 'days_since_prior_order',\n",
       "       'order_hour_of_day', 'user_avg_spacing', 'streak_nan', 'up_buy_streak',\n",
       "       'up_n5_n_buys', 'up_n5_buy_ratio', 'up_atco_sum', 'up_atco_avg',\n",
       "       'prod_total_mkt_share', 'prod_total_mkt_share_log', 'aisle_total_sales',\n",
       "       'prod_aisle_mkt_share', 'prod_aisle_mkt_share_log', 'streak_abs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pickled feature DF generated by feature_engineering_1.ipynb\n",
    "X = pd.read_pickle(\"./pickle/X_F.pickle\")\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unnecessary-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-moment",
   "metadata": {},
   "source": [
    "## Let's start with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = split_users(X, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(n_estimators=500, max_depth=8, max_features=8, n_jobs=-1)\n",
    "# rf_fit = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = rf_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/rf_fit.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(rf_fit, pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-measure",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_users(X, subset=0.02, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = XGBClassifier(objective='binary:logistic',\n",
    "#                           use_label_encoder=False,\n",
    "#                           eval_metric='logloss',\n",
    "#                           random_state=54,\n",
    "#                           learning_rate=0.01,\n",
    "# )\n",
    "\n",
    "# params = {\n",
    "#     'max_depth': [7, 8, 9],\n",
    "#     'n_estimators': [400, 500],\n",
    "#     'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "#     'min_child_weight': [7, 8, 9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(\n",
    "#     estimator = estimator,\n",
    "#     param_grid = params,\n",
    "#     verbose=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# grid_xgb_fit = grid_search.fit(X_train, y_train)\n",
    "# print(\"The best parameters are: \\n\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/grid_xgb_fit.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(grid_xgb_fit, pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-affairs",
   "metadata": {},
   "source": [
    "## Grid Search Results\n",
    "\n",
    "**On a different VM, I used grid search to tune min_child_weight and colsample_bytree parameters. Here was the grid & results:**\n",
    "\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=54,\n",
    "                          max_depth=3,\n",
    "                          learning_rate=0.01,\n",
    "                          n_estimators=500\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'min_child_weight': range(1, 10, 1),\n",
    "    'colsample_bytree': [.6, .7, .8, .9, 1.0]\n",
    "}\n",
    "\n",
    "# results\n",
    "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
    "The best parameters are: \n",
    " {'colsample_bytree': 0.7, 'min_child_weight': 8}\n",
    "CPU times: user 1h 47min 29s, sys: 1.55 s, total: 1h 47min 30s\n",
    "Wall time: 10min 54s\n",
    "\n",
    "```\n",
    "\n",
    "**Below were the parameters for our grid search.**\n",
    "```\n",
    "estimator = XGBClassifier(objective='binary:logistic',\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss',\n",
    "                          random_state=54,\n",
    "                          learning_rate=0.01,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'max_depth': [7, 8, 9],\n",
    "    'n_estimators': [400, 500],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'min_child_weight': [7, 8, 9]}\n",
    "```\n",
    "\n",
    "**And the results:**\n",
    "\n",
    "```\n",
    "Best results are:\n",
    "{'colsample_bytree': 0.8, 'max_depth': 7, 'min_child_weight': 9, 'n_estimators': 400}\n",
    "CPU times: user 21h 33min 24s, sys: 18.2 s, total: 21h 33min 42s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-tribune",
   "metadata": {},
   "source": [
    "Great. Now we have our parameters. Let's run the model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_users(X, subset=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier(colsample_bytree=0.8,\n",
    "                    min_child_weight=9,\n",
    "                    n_estimators=400,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.009,\n",
    "                    eval_metric='logloss',\n",
    "                    verbosity=3,\n",
    "                    use_label_encoder =False)\n",
    "\n",
    "xgb_fit = xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/xgboost_fit_all.pickle\", \"wb\") as pfile:\n",
    "        pickle.dump(xgb_fit, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/xgboost_fit_all.pickle\", \"rb\") as pfile:\n",
    "#     xgb_fit = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = xgb_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_all_score = f1_score(y_test, preds_all)\n",
    "xgb_all_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-longitude",
   "metadata": {},
   "source": [
    "`xgb_fit` F-1 score: `0.2722773221438108`\n",
    "* this is before the following feature changes:\n",
    "    * remove `streak`\n",
    "    * remove department features\n",
    "    * add `prod_reorder_pct`\n",
    "\n",
    "After those changes, our score is: ``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-nevada",
   "metadata": {},
   "source": [
    "Now let's try a couple more, each time without one of the following features that may/may not be helpful\n",
    "* log features\n",
    "* streak\n",
    "* streak_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_2 = X.drop(columns=[\"prod_dpt_mkt_share_log\", \"prod_aisle_mkt_share_log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = split_users(X_2, subset=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_no_log = XGBClassifier(colsample_bytree=0.8,\n",
    "#                                min_child_weight=9,\n",
    "#                                n_estimators=400,\n",
    "#                                max_depth=7,\n",
    "#                                learning_rate=0.009,\n",
    "#                                eval_metric='logloss',\n",
    "#                                verbosity=3,\n",
    "#                                use_label_encoder =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_fit_no_log = xgb_no_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_fit_no_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_no_log = xgb_fit_no_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_no_log_score = f1_score(y_test, preds_no_log)\n",
    "# xgb_no_log_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-agent",
   "metadata": {},
   "source": [
    "`xgb_no_log_score` F-1 score: `0.2720328996646104`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/xgboost_fit_no_log.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(xgb_fit_no_log, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_3 = X.drop(columns=\"streak\")\n",
    "# X_train, X_test, y_train, y_test = split_users(X_3, subset=False, test_size=0.2)\n",
    "\n",
    "# xgb_no_streak = XGBClassifier(colsample_bytree=0.8,\n",
    "#                                min_child_weight=9,\n",
    "#                                n_estimators=400,\n",
    "#                                max_depth=7,\n",
    "#                                learning_rate=0.009,\n",
    "#                                eval_metric='logloss',\n",
    "#                                verbosity=3,\n",
    "#                                use_label_encoder =False)\n",
    "\n",
    "# xgb_fit_no_streak = xgb_no_streak.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_no_streak = xgb_fit_no_streak.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_score_no_streak = f1_score(y_test, preds_no_streak)\n",
    "# xgb_score_no_streak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-dress",
   "metadata": {},
   "source": [
    "`xgb_score_no_streak` F-1 score: `0.2726189454819306`\n",
    "\n",
    "Our best score yet! Next time, we'll run it again without this column. It is redundant, as it's captured in `xgb_score_no_streak_abs`, which is well-accompanied by `streak_nan` and `up_buy_streak`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/xgboost_fit_no_streak.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(xgb_fit_no_streak, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_4 = X.drop(columns=\"streak_abs\")\n",
    "# X_train, X_test, y_train, y_test = split_users(X_4, subset=False, test_size=0.2)\n",
    "\n",
    "# xgb_no_streak_abs = XGBClassifier(colsample_bytree=0.8,\n",
    "#                                min_child_weight=9,\n",
    "#                                n_estimators=400,\n",
    "#                                max_depth=7,\n",
    "#                                learning_rate=0.009,\n",
    "#                                eval_metric='logloss',\n",
    "#                                verbosity=3,\n",
    "#                                use_label_encoder =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_fit_no_streak_abs = xgb_no_streak_abs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_no_streak_abs = xgb_fit_no_streak_abs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_score_no_streak_abs = f1_score(y_test, preds_no_streak_abs)\n",
    "# xgb_score_no_streak_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-career",
   "metadata": {},
   "source": [
    "`xgb_score_no_streak_abs` F-1 score: `0.2711982545843419`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/xgboost_fit_no_streak_abs.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(xgb_fit_no_streak_abs, pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-deployment",
   "metadata": {},
   "source": [
    "And now we'll do the same without `colsample_bytree` and `min_child_weight`. We'll call this model `xgb_fit_2`.\n",
    "\n",
    "We'll then compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_5 = X.drop(columns=['prod_dpt_mkt_share', 'prod_dpt_mkt_share_log', 'dpt_total_sales'])\n",
    "# X_train, X_test, y_train, y_test = split_users(X_5, subset=False, test_size=0.2)\n",
    "\n",
    "# xgb_no_dpt = XGBClassifier(colsample_bytree=0.8,\n",
    "#                                min_child_weight=9,\n",
    "#                                n_estimators=400,\n",
    "#                                max_depth=7,\n",
    "#                                learning_rate=0.009,\n",
    "#                                eval_metric='logloss',\n",
    "#                                verbosity=3,\n",
    "#                                use_label_encoder =False)\n",
    "\n",
    "\n",
    "# xgb_fit_no_dpt = xgb_no_dpt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"models/xgb_fit_no_dpt.pickle\", \"wb\") as pfile:\n",
    "#         pickle.dump(xgb_fit_no_dpt, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_no_dpt = xgb_fit_no_dpt.predict(X_test)\n",
    "\n",
    "# xgb_score_no_dpt = f1_score(y_test, preds_no_dpt)\n",
    "# xgb_score_no_dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-charleston",
   "metadata": {},
   "source": [
    "`xgb_score_no_dpt` F-1 score: `0.27230669439304045`\n",
    "\n",
    "**Now let's test out removing the `up_atco` (user-purchase add-to-cart-order) columns**\n",
    "\n",
    "We'll start with just dropping the `up_atco_sum` column, since that's the one that makes the least sense (to me, anyways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civil-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    X_train sample size: 6782401\n",
      "    X_test sample size: 1692260\n"
     ]
    }
   ],
   "source": [
    "X_6 = X.drop(columns='up_atco_sum')\n",
    "X_train, X_test, y_train, y_test = split_users(X_6, subset=False, test_size=0.2)\n",
    "\n",
    "xgb_no_atco_sum = XGBClassifier(colsample_bytree=0.8,\n",
    "                               min_child_weight=9,\n",
    "                               n_estimators=400,\n",
    "                               max_depth=7,\n",
    "                               learning_rate=0.009,\n",
    "                               eval_metric='logloss',\n",
    "                               verbosity=3,\n",
    "                               use_label_encoder =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-sequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:16:53] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/gbm/gbtree.cc:146: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[07:16:53] DEBUG: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/gbm/gbtree.cc:154: Using tree method: 1\n",
      "[07:16:53] DEBUG: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/gbm/gbtree.cc:119: Using updaters: grow_histmaker,prune\n",
      "[07:16:53] ======== Monitor: TreePruner ========\n",
      "[07:16:59] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:03] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:07] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:11] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:15] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:19] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:24] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:28] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:32] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:36] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:40] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:44] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:49] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:53] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:17:57] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[07:18:01] INFO: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/tree/updater_prune.cc:101: tree pruning end, 254 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    }
   ],
   "source": [
    "xgb_fit_no_atco_sum = xgb_no_atco_sum.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/xgb_fit_no_atco_sum.pickle\", \"wb\") as pfile:\n",
    "        pickle.dump(xgb_fit_no_atco_sum, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_no_atco_sum = xgb_fit_no_atco_sum.predict(X_test)\n",
    "\n",
    "xgb_score_no_atco_sum = f1_score(y_test, preds_no_atco_sum)\n",
    "xgb_score_no_atco_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-grenada",
   "metadata": {},
   "source": [
    "And now let's try with `atco_sum`, but without `atco_avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_7 = X.drop(columns='up_atco_avg')\n",
    "X_train, X_test, y_train, y_test = split_users(X_7, subset=False, test_size=0.2)\n",
    "\n",
    "xgb_no_atco_avg = XGBClassifier(colsample_bytree=0.8,\n",
    "                               min_child_weight=9,\n",
    "                               n_estimators=400,\n",
    "                               max_depth=7,\n",
    "                               learning_rate=0.009,\n",
    "                               eval_metric='logloss',\n",
    "                               verbosity=3,\n",
    "                               use_label_encoder =False)\n",
    "\n",
    "xgb_fit_no_atco_avg = xgb_no_atco_avg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/xgb_no_atco_avg.pickle\", \"wb\") as pfile:\n",
    "        pickle.dump(xgb_fit_no_atco_avg, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_no_atco_avg = xgb_fit_no_atco_avg.predict(X_test)\n",
    "\n",
    "xgb_score_no_atco_avg = f1_score(y_test, preds_no_atco_avg)\n",
    "xgb_score_no_atco_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-sitting",
   "metadata": {},
   "source": [
    "And now without both ATCO columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_8 = X.drop(columns='up_atco_avg')\n",
    "X_train, X_test, y_train, y_test = split_users(X_8, subset=False, test_size=0.2)\n",
    "\n",
    "xgb_no_atco = XGBClassifier(colsample_bytree=0.8,\n",
    "                               min_child_weight=9,\n",
    "                               n_estimators=400,\n",
    "                               max_depth=7,\n",
    "                               learning_rate=0.009,\n",
    "                               eval_metric='logloss',\n",
    "                               verbosity=3,\n",
    "                               use_label_encoder =False)\n",
    "\n",
    "xgb_fit_no_atco = xgb_no_atco.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/xgb_no_atco.pickle\", \"wb\") as pfile:\n",
    "        pickle.dump(xgb_fit_no_atco, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_no_atco = xgb_fit_no_atco.predict(X_test)\n",
    "\n",
    "xgb_score_no_atco = f1_score(y_test, preds_no_atco)\n",
    "xgb_score_no_atco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-graph",
   "metadata": {},
   "source": [
    "# ELLIOT - MONDAY MORNING\n",
    "\n",
    "Check above scores, **record them in markdown**, then make the necessary feature changes and re-run the model.\n",
    "\n",
    "Then, take the results and re-run model_analysis.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-place",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
