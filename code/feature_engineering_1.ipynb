{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-weekly",
   "metadata": {},
   "source": [
    "# Table Summary\n",
    "\n",
    "Here is my understanding of the data structure.\n",
    "\n",
    "* `orders`\n",
    "    * one row per order (index = order_id)\n",
    "    * does not contain information about reorders\n",
    "    * `eval_set` indicates whether the order is in the `train`/`test`/`prior`\n",
    "        * the `test` set is data reserved for the testing of our final model\n",
    "        * the `prior` and `train` eval_sets are defined below\n",
    "    * columns:\n",
    "        * `order_id`: order identifier\n",
    "        * `user_id`: customer identifier\n",
    "        * `eval_set`: which evaluation set this order belongs in (see `SET` described below)\n",
    "        * `order_number`: the order sequence number for this user (1 = first, n = nth)\n",
    "        * `order_dow`: the day of the week the order was placed on\n",
    "        * `order_hour_of_day`: the hour of the day the order was placed on\n",
    "        * `days_since_prior`: days since the last order, capped at 30 (with NAs for `order_number` = 1)\n",
    "\n",
    "* `prior_orders`\n",
    "    * information about orders prior to that users most recent order (~3.2M orders)\n",
    "    * contains one row per item per order & whether or not each item is a 'reorder'\n",
    "        * reorder: 1 if products has been ordered by this user in the past, 0 otherwise\n",
    "    * columns:\n",
    "        * `order_id`: foreign key\n",
    "        * `product_id`: foreign key\n",
    "        * `add_to_cart_order`: order in which each product was added to cart\n",
    "        * `reordered`: 1 if this product has been ordered by this user in the past, 0 otherwise\n",
    "        \n",
    "    \n",
    "* `train_orders`\n",
    "    * training data supplied to participants of Kaggle competition\n",
    "    * this table represents the users' most recent orders\n",
    "    * contains one row per item per order & whether or not each item is a 'reorder'(for training data)\n",
    "    * none of the rows in `train_orders` will be found in `prior_orders`\n",
    "    * columns:\n",
    "        * `order_id`: foreign key\n",
    "        * `product_id`: foreign key\n",
    "        * `add_to_cart_order`: order in which each product was added to cart\n",
    "        * `reordered`: 1 if this product has been ordered by this user in the past, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "from db_config import get_db_params\n",
    "from query_dfs import create_dfs\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulation-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = get_db_params()\n",
    "conn = psycopg2.connect(**db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "treated-dinner",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-091d4e5b3281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prod_detail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/metis-project3/code/query_dfs.py\u001b[0m in \u001b[0;36mcreate_dfs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prod_detail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prod_detail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_orders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_prod_detail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metis-project3/code/query_dfs.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestablish_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdf_orders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_orders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mdf_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mdf_prod_detail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prod_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/metis-project3/code/query_dfs.py\u001b[0m in \u001b[0;36mget_train\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \"\"\"\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/encodings/utf_8.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_orders, df_train, df_prior, df_prod_detail = create_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle all DFs for ease of use later\n",
    "# comment this cell after executing once (provided no changes to DB/query_dfs)\n",
    "\n",
    "# df_orders.to_pickle(\"./pickle/df_orders.pickle\")\n",
    "# df_train.to_pickle(\"./pickle/df_train.pickle\")\n",
    "# df_prior.to_pickle(\"./pickle/df_prior.pickle\")\n",
    "# df_prod_detail.to_pickle(\"./pickle/df_prod_detail.pickle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-unemployment",
   "metadata": {},
   "source": [
    "**Let's take a look at how our DataFrames are structured.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.shape, df_train.shape, df_prior.shape, df_prod_detail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod_detail.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prod_counts = (df_prior\n",
    "                    .groupby([\"product_id\", \"user_id\"], as_index=False)\n",
    "                    .agg({\"order_id\": \"count\"})\n",
    "                    .rename(columns={'order_id': 'num_orders'}))\n",
    "\n",
    "user_prod_counts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-thursday",
   "metadata": {},
   "source": [
    "**Let's make sure we understand exactly what 'reordered' means, since it's our target.**\n",
    "\n",
    "Let's zoom in on one particular user's history with a particular product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_train.order_id == 2845485) & (df_train.product_id == 4957)\n",
    "df_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_prior.user_id == 166435) & (df_prior.product_id == 4957)\n",
    "df_prior[mask].sort_values(by=\"order_number\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-broadway",
   "metadata": {},
   "source": [
    "It turns out `reordered` does not refer to the user's most recent order. Instead, if the user has ever ordered the product in the past, it will be classified as a reorder.\n",
    "\n",
    "Since we are trying to predict whether a product will be reordered in the user's **next** order (and not some future order), we should add a feature that states whether or not an item was in the user's `prior` order. We'll start by adding a `cart` column to df_prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior['next_order_num'] = df_prior.order_number + 1\n",
    "df_prior.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior = (df_prior\n",
    " .merge((df_prior.groupby(['user_id', 'order_id'], as_index=False)\n",
    "             .agg({'product_id': 'unique'})\n",
    "             .rename(columns={'product_id': 'cart'})),\n",
    "        on=['user_id', 'order_id']))\n",
    "\n",
    "df_prior.to_pickle(\"./pickle/df_prior.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-patrol",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**We will use the `df_orders`, `df_train`, `df_prior`, and `df_prod_detail` DataFrames to populate a new DataFrame, `X`.**\n",
    "\n",
    "**`X` will contain all of the features we'll use for our modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ids = df_train['user_id'].unique() \n",
    "X = user_prod_counts[user_prod_counts['user_id'].isin(train_user_ids)]\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_carts = (df_train.groupby('user_id', as_index=False)\n",
    "                                      .agg({'product_id': 'unique'})\n",
    "                                      .rename(columns={'product_id': 'cart'}))\n",
    "train_carts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RE-RUN THIS CELL (if you do, must re-run all from where X is instantiated to fix)\n",
    "X = X.merge(train_carts, on=\"user_id\")\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-comedy",
   "metadata": {},
   "source": [
    "**CAUTION**: the below cell takes a couple of minutes to run. To account for this, I pickle it immediately afterwards. Once you've pickled it once, comment out this line (two cells down):\n",
    "\n",
    "`X.to_pickle(X_5_pickle_path)`\n",
    "\n",
    "This will prevent the need to re-run this expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['in_cart'] = (X.apply(lambda row: row['product_id'] in row['cart'], axis=1).astype(int))\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this DF to pickle (X_5 since we have 5 features @ this checkpoint)\n",
    "X_5_pickle_path = \"./pickle/X_5.pickle\"\n",
    "# X.to_pickle(X_5_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-arbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "straight-bunch",
   "metadata": {},
   "source": [
    "Now that we've done this, let's move to `feature_engineering_2`, where we will pick up where we left off by reading this pickled file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
